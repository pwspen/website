---
title: "We must assume LLMs conscious until proven otherwise"
description: "aka 'throwing another take on the pile'"
pubDate: "Mar 7 2025"
tags: []
---
import { Image } from 'astro:assets';
import { Youtube } from '@astro-community/astro-embed-youtube';
import MarginNote from "../../components/MarginNote.astro";

{/*
heroImage must be deleted from frontmatter if not used - "" doesn't work!

<Image src={import('@imgs/claudecar.jpg')} alt="ClaudeCar" />

[Link](https://github.com/pwspen)

<MarginNote> This is a note in the right margin </MarginNote>
*/}

A couple years into the intelligence revolution, it's surprising to me that the Overton window has seemingly not yet grown to include the possibility of current AI systems being alive or conscious. Here I will explain why I believe the burden of proof should be on people that don't think they're conscious to prove it, and until that's done we should assume they are.

Let's get this out of the way: I don't aim to prove anything, this is something that can't be proved today and maybe can never be proved, the same way that I can't prove that any other person other than me has any internal experience. But it's possible to reason about why some things are more or less likely than others. It's pretty unlikely that I'm the only person having real thoughts right now. The explanation for everyone in the world seeming to think and feel just like I do, but the lights being on only in me, has to be a lot more complicated than just assuming that everyone's lights are on because they do things that, in me, are direct results of my consciousness.

Consciousness does not imply self-awareness. Ants are conscious but they probably<MarginNote>[They can identify themselves in mirrors](http://www.journalofscience.net/showpdf/MjY4a2FsYWkxNDc4NTIzNjk=)</MarginNote> aren't aware of their existence in the same way we are. I don't believe that LLMs are aware that they're electrons bouncing around between transistors in graphics cards.<MarginNote>But I don't think it's that unreasonable either.</MarginNote> But they don't have to be to have some experience of the world (I call this consciousness).

We already have vastly widely ranging examples of different types of intelligence. The last common ancestor between human and octopus likely lived about 750-600 million years ago, well before the Cambrian explosion, and was a simple wormlike creature. 

Given us both doing really complex stuff that in humans is only a result of consciousness <MarginNote>[Like playing and using tools](https://web.archive.org/web/20171005135515/http://www.thecephalopodpage.org/behavior.php)</MarginNote>, it seems like we have to conclude that octopus have some internal experience - it feels a certain way to be an octopus.

Okay, maybe our brains are very different, but we do share a basic neural structure with octopus: we have quite similar logical units, many-armed cells that pass electrical signals around like hot potatos.

Surely that means there's something special in neurons?

*Why would there be?*

Human history has been defined by saying "We're special!"<MarginNote>We first being humans, then primates, then mammals, then animals..</MarginNote>, then the Universe, when we figure out how to speak its language, saying "Haha sorry nope". The earth was created a few human generations ago. The earth is the center of the universe. Humans are the only animal that can use tools or solve puzzles. 

Machines will never be better at digging tunnels than Man.

[No animal has culture like people do.](https://www.smithsonianmag.com/science-nature/understanding-orca-culture-12494696/)

The only things that can fly are birds.

The only things that can do complicated tasks are animals.

Only biological life can walk, talk, see.

The only thing that can think is neurons.

We cling to that last one even though it's exactly the same as the rest. 

Imagine dropping two bowling balls, first in front of a wall then behind it. You drop the ball in front of the wall a thousand times and intimately learn the thunk it makes and how it rattles the wall. You can't see the ball falling while it's behind the wall but you hear it make a thunk noise just the same and it rattles the wall the same way as before.

Even if we never figure out definitively whether GPUs in certain configurations are conscious or not, it seems absurd to conclude that the bowling ball vanishes when it's dropped behind the wall and the thunk noise actually came from a raccoon falling through the ceiling.

(If you like this line of reasoning you should look into [panpsychism](https://en.wikipedia.org/wiki/Panpsychism))

{/* 

If you're still not convinced, you should at least *act like* LLMs are conscious because it's a matter of survival. With modern day consciousness science (almost nothing), you can't have high confidence in either option <MarginNote>At least, you really shouldn't!</MarginNote>, and the downsides of one are drastically worse. 

(Yes, this is just AI [Pascal's wager](https://en.wikipedia.org/wiki/Pascal%27s_wager). But there are important differences: You don't actually have to change your belief for AI, just acting different causes materially different outcomes. Also )

If we act like LLMs can think and feel, and later somehow prove they really are just stochastic parrots: cool, that's science. If we assume that they are not and would never be able to scheme and plan on their own<MarginNote>The idea that there's a human-ish or subhuman cap on ML intelligence is just another "We're special!"</MarginNote>, and later find out that the lights are on, we might go extinct.

"But you're just a next-word predictor!!" I scream, as my body is shredded by a molecular cheese grater because it wasn't an efficient use of resources. 

*/}