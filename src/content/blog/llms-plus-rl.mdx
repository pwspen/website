---
title: "Why LLMs + RL Is All You Need (to create superintelligence)"
description: ""
pubDate: "Feb 12 2025"
tags: ["essay", "ai"]
---
import { Image } from 'astro:assets';
import { Youtube } from '@astro-community/astro-embed-youtube';
import MarginNote from "../../components/MarginNote.astro";

{/*
<Image src={import('@imgs/claudecar.jpg')} alt="ClaudeCar" />

[Link](https://github.com/pwspen)

<MarginNote> This is a note in the right margin </MarginNote>
<MarginNote> </MarginNote>
*/}
{/* */}

In the last six months, a new way of training AI has proven itself, parting the mists around the next couple years of AI advancement, and there's now a clear path to push capabilities far beyond their current levels (which there arguably wasn't six months ago).

This training method is extremely promising - it's shortened my personal AI timelines by 2x. Its potential is also completely terrifying. I'm looking at the cost of land far from population centers and trying to shift careers into AI safety. Below, I hope to succinctly explain the core ideas behind the advancement (reinforcement learning (RL) + LLMs), and why this might be your sign to start paying attention to AI if you've been a skeptic thus far.

All of the recent reasoning models (o1, o3, R1) were trained using reinforcement learning, while previous LLMs relied on unsupervised learning <MarginNote>for the bulk of their capabilities. RLHF is a thing, but it tunes to human preferences while reasoning RL tunes to *truth* - a big difference!</MarginNote>. Unsupervised learning is memorizing patterns in data to find the likeliest next data point. Reinforcement learning is trying things and doing more of what works.

Imagine a chef that has learned to cook by watching other chefs make dishes, compared to a chef that makes dishes and tastes them. Which will serve a better steak? One learns by watching what other people do. There's clearly a skill cap in this case. The other learns by doing things and verifying whether or not (or how well <MarginNote>But this is more susceptible to [reward hacking](https://en.wikipedia.org/wiki/Reward_hacking). Deepseek R1 specifically chose a binary "worked or didn't" reward to combat reward hacking.</MarginNote>) it worked. Here there's nothing of the sort.

It's really the combination of RL and unsupervised learning, though, that has so much potential. To do RL on a given task, you need two things: To be able to simulate the task decently, and a clear definition of "better", the metric that you want the training process to optimize. How does this apply to the task of reasoning? For technical problems like math and code, defining better is easy: you can just check the answer. If 2+2 doesn't equal 4, it's wrong. If a function to sort a list returns [3, 1, 2], it's wrong.

But until recently, we didn't have a way to simulate reasoning. This is what LLMs (powered by unsupervised learning) have given us: a simulacrum of the human thought process. It doesn't matter one bit to RL if it's the same things happening underneath, it just matters if they look about the same. And they do!

Strikingly, it appears that the reasoning model training process is largely about aligning the bits of the LLM that already knew about reasoning to make 'reasoning' the default state, instead of training the model to reason from scratch. Humans have something like this, too: You can concentrate hard on something to make sure you get it right, but it's exhausting to do so for long periods. Reasoning training puts the language model in this state permanently, at no extra cost <MarginNote> No extra cost *per token*. Models learn to lengthen their responses to improve their performance, so the cost per task is higher. </MarginNote> (LLMs don't get tired). Because it's about eliciting already-present thought patterns more than creating new ones, performant reasoning models can be [trained on](https://github.com/simplescaling/s1) [shoestring budgets](https://github.com/Jiayi-Pan/TinyZero).

An important difference between training methods is that unsupervised learning training works on the token level: The model learns from each token that it predicts. The reasoning model training process usually uses entire tasks, not tokens, as single training 'events'. This is less efficient <MarginNote> The [Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) always wins! </MarginNote>, but it optimizes for what we actually care about. You don't care if a model gets one word right that seems like a plausible part of the answer to your question. You do care if it answers your question correctly. Not checking the model's every token also gives it more freedom. <MarginNote>This extra freedom is why pure RL, like R1-Zero, can have pretty wonky outputs, while still having high accuracy.</MarginNote>

Another distinction between unsupervised training and RL: The internet has been thoroughly scraped for all of its text at this point to provide data for unsupervised learning, and the focus has shifted more towards curation of that text and generation of new, high quality text to teach smaller models. But when you're training a model to reason, the data you need is *questions whose answers are known* (or can be verified). <MarginNote> Here the lines between supervised learning and RL blur. </MarginNote> And it seems like we've barely scratched the surface on that. Every scientific paper ever published - millions and millions - probably has a couple verifiable questions.

One of the most interesting insights from Deepseek's R1 was that the more complicated stuff they tried to optimize the reasoning training process worked *worse* <MarginNote>The big problem was reward hacking, the bane of all RL.</MarginNote> than the simple stuff. They settled on a very simple reward model, with most of the reward being for getting the task right or not, and a smaller portion when proper formatting is followed.

So where does this leave us?

We have a new stage of LLM training that:
1. Optimizes for "truth" <MarginNote> Or at least, something much closer to it than we could optimize for previously. </MarginNote> across a wide range of knowledge tasks
2. Opens up new training data sources (any objectively verifiable question is game)
3. Has no apparent capability ceiling - Just look at AlphaGo!

For a while, it was unclear if LLMs would be the thing getting us to AGI and ASI. Based on the above points, it seems very likely to me that *LLMs + RL is enough to get to the point where we're not the ones making the decisions anymore.*

I can't predict the future, but I can say with extremely high certainty that, in the time it's taken to write this post, thousands or millions of technical problems have been tackled by silicon minds in the datacenters of Meta, OpenAI, Google, and all the rest. They learn from those they get correct, and from those they don't. This blistering pace is the slowest that they will learn for the rest of history. And right now is the stupidest they'll ever be, and they're already solidly above the average person, or even average college grad.

The age of humans is coming to a close.

{/*
First draft:

Aka "Why Deepseek R1 shows we're much closer to ASI than we thought"

Reinforcement learning drove many of the advances in pre-LLM AI. It's by far the most common method of training robots to move, and it also drove important narrow-superintelligence advancements like chess and Go. Now it looks like it might be the most important invention in human history, driving us to AGI and beyond.

Unsupervised learning is the main technology behind today's language model boom. A large fraction of the text on the internet is shoveled into an optimizer algorithm trained to sculpt a different algorithm (for most LLMs, a transformer) to predict the next word.

This way of doing things has an obvious limitation. Training an AI on stuff written by humans is going to make it act like a human. You can make it act like a smart human by being very careful with what exact data you shovel into it, but there's no way to make it act like a super-Einstein because we don't have data on what a super-Einstein would act like. We probably don't even have enough data on what Einsteins act like to make a model that's a good approximation of them.

What if you didn't need *any* data?

This is the evergreen promise of reinforcement learning. "Reinforcement learning" really just means "try things and do more of what works better". The only problem is figuring out what "better" actually means.

The success of reinforcement learning in a domain or task, given enough compute (a big assumption!), is largely dependent on two things:
1. How well it can be simulated
2. How well "better" can be defined

If both can be done well, you can probably make an AI that does the task at least as well as a human (in robotics, the hardware also has to be good enough.)

Boardgames and videogames fulfill both of these very well. We invented all the rules and it is possible to win, so that's what better is. Usually you can even tell how close you are to winning - there's our signal for "better".

Robotics is tougher. We can simulate physics, but it's never quite perfect. The real robots we build have things like machining tolerances and sensor inaccuracy that puts an upper limit on how well we can simulate them. And most of the time there's parameters that contribute significantly to physics that are hard or impossible to measure. For example, what's the coefficient of friction of a certain type of rubber in a certain shape with a certain amount of weight on it, on a certain type of surface? We can only approximate.

This is known as the "reality gap". It causes serious problems for robots and there's no silver bullet. Sometimes your simulation has to be tweaked, sometimes you have to do some training on real-world data.

The second problem - defining "better" - is much worse, and it's at the heart of alignment and safety research that extends far beyond any single algorithm. The core issue is that, as a task gets more complex, it gets vastly more difficult to create a "goodness" function (in RL, reward function) that maps decently to our built-in function for knowing how good a given state is for accomplishing the task.

A concrete example: I trained a quadruped using reinforcement learning. The goal was to get it to walk, so the main components of its goodness function were the height of the body, and the forward velocity. [But it ended up doing something that's definitely not walking!](/headwalk.html)

By walking on its head, it greatly simplified the problem of coordinating four limbs into coordinating three with two of them moving in sync. Pretty smart! And really, that's the problem: If there's *any* way for the training process to find a way to get good reward with less effort, it will probably find it, and it probably won't do what you intended. In my case, the headwalking was after several iterations of reward function tweaking. At first, it just did yoga and didn't even move because it got too much reward for just staying alive.

A much more dangerous situation is when there's gobs and gobs of reward available by doing something. This is the classic stamp or paperclip collector.

So back to training an Einstein. We need to simulate the thinking process and we need some way of determining which thoughts are smarter.

Until a couple years ago, we didn't have any way - not even close - to simulate human thinking at a reasonable fidelity. But that's exactly what language models have given us. 

Interstingly, unlike robotics, there is no simulation-to-reality gap for language models, because the simulation (all the text fed to the models) *is* reality. So there's one problem down.

And much more recently, the first tentative steps have been made into the second problem: defining what it means for a thought or thinking process to be better.

Reasoning models are the culmination of these advancements. Unlike any previous LLMs <MarginNote> RLHF is not very similar to reasoning RL. To me, the important part about RL is that the learning trickles down from validated true facts of reality, in a more direct way than any other type of training. In RLHF, the learning trickles down from human preference. Reasoning RL can *improve* LLM reasoning, RLHF can only make it so humans prefer it more. Even if humans were specifically trying to select good reasoning patterns (not at all what RLHF is usually used for), human discernment between good and bad reasoning will get worse as the reasoning quality approaches and passes that of humans. </MarginNote> they have a step in training where the goal is entirely divorced from copying human text. Instead, it's to *be better at thinking*.

So how do you evaluate if one thought is better than another? The current thinking is tied to verifiability: If one way of thinking is better at producing artifacts with verifiable properties, it's better.

An artifact is a number, an equation, a graph, code. Anything that's easy to check whether it follows rules. You might check that a number is equal to what you expected, or that an equation goes through certain points. For code, there's a lot you can check: Does it work for all of the edge cases you can think of? How fast is it? How many lines is it? When combined with a thinking process that can write code or do math, these things are signals that can be used to steer that process.

Deepseek R1 was released recently - the first large and open reasoning model. You've heard of it (and so have your mom and your dog and your pet rock) but IMO the most interesting things about it didn't make the headlines.

As one of the first performant reasoning models that also published the training process, it's hard to compare it to other reasoning models. But notably, R1 relied fairly heavily on pure reinforcement learning. The first stage of its training was taking a regular ol' language model, giving it a simple prompt, and doing reinforcement learning with a simple reward model. Pretty much exactly as described above. Later non-RL stages were either to make the output more human friendly, or to improve the model's performance on tasks outside of reasoning.

It's pretty startling that something so simple works so well. In fact, they tried more complicated reward functions, and found they performed worse.

It's a resounding confirmation of the [bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) in machine learning: compute usually wins over fancy ideas. (DeepSeek agrees: "... advancing beyond the boundaries of intelligence may still require more powerful base models and larger- scale reinforcement learning.") This new type of training is much more obvious than the last big advancement it's built upon (transformers).

It's also, at least to me, absolutely terrifying that it works so well.

The problem mentioned above of reinforcement learning finding ways to "cheat the system" is known as specification gaming or reward hacking. R1 has a pretty simple reward function (1 if correct 0 if not, plus up to 0.5 for following a certain answer format and staying in a single language), and even still it seems like there was little or no problem with reward hacking. Why not?

Well, there's not much to hack when most of your reward comes from simply being correct, or not, no in-between (DeepSeek tried some reward models that allowed for in-between results and found they encouraged reward hacking). In some ways, reasoning is vastly simpler than robotics because it can be *correct*. There is a vastly wider range of possible results when you're walking or picking up an apple or doing a backflip, and it's not really possible to specify a single *correct* way of doing any of those things. Basically, reasoning and reinforcement learning are just a great pair.

Points:
- The promise of RL has always been that you don't need any data, just a simulation and a way to define "better"
- Until recently, we didn't have a way to simulate or copy (it doesn't matter which) (at any reasonable fidelity) the human thinking / reasoning process, but LLMs gave us a great one
- Text-based reasoning doesn't have a sim2real gap, and it also seems like it's possible to largely sidestep reward hacking with a binary reward function (which is only made possible by starting with a somewhat capable base model before RL)
- RLHF is not at all similar to this new type of reasoning RL!
- A lot of the more complicated stuff they tried worked worse than the simpler process/reward that they ended up going with
- R1 is a resounding confirmation of the bitter lesson, as DeepSeek themselves point out
- There are no compelling reasons remaining that current methods (transformer LLMs + RL + tons of compute) won't be able to get us to superintelligence, unlike plain unsupervised learning
- There is lots of readily available verifiable problems waiting to be turned into reasoning training data: a couple verifiable reasoning problems can probably be generated from any paper. Optimizing software and verifying whether it was improved or not (and by how much) is another easy one.
*/}
